import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import re
import os
import unicodedata
import sys
import argparse
import random

# ==========================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
DEFAULT_TARGET_DATE = "20250101" 
MAX_RACES = 5 

# å½è£…ç”¨User-Agentãƒªã‚¹ãƒˆ
UA_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
]

def log(msg):
    """ãƒ­ã‚°ã‚’å³æ™‚å‡ºåŠ›ï¼ˆflush=Trueï¼‰"""
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def clean_text(text):
    if not text: return ""
    text = unicodedata.normalize('NFKC', str(text))
    return text.replace("\n", "").replace("\r", "").replace(" ", "").replace("Â¥", "").replace(",", "").strip()

def get_soup(url, description="ãƒšãƒ¼ã‚¸"):
    """HTMLå–å¾—ï¼ˆUser-Agentãƒ©ãƒ³ãƒ€ãƒ åŒ– & ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·ï¼‰"""
    for i in range(1, 4): # 3å›ãƒªãƒˆãƒ©ã‚¤
        try:
            # æ¯å›UAã‚’å¤‰ãˆã‚‹
            headers = {'User-Agent': random.choice(UA_LIST)}
            
            # timeoutã‚’ 30ç§’ ã«å»¶é•·
            res = requests.get(url, headers=headers, timeout=30)
            res.encoding = res.apparent_encoding
            
            if res.status_code == 200:
                if "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in res.text:
                    log(f"     âš ï¸ {description}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                    return None
                return BeautifulSoup(res.text, 'html.parser')
            else:
                log(f"     âš ï¸ {description}: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {res.status_code} (Wait 5s...)")
                time.sleep(5)
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’çŸ­ãè¡¨ç¤º
            err_msg = str(e)
            if "read timeout" in err_msg.lower():
                err_msg = "Read Timeout (å¿œç­”ãªã—)"
            log(f"     âŒ {description}: {err_msg} (Wait 5s...)")
            time.sleep(5)
            
    log(f"     ğŸ’€ {description}: å–å¾—å¤±æ•—ï¼ˆ3å›è©¦è¡Œï¼‰")
    return None

def scrape_race(jcd, rno, date_str):
    """1ãƒ¬ãƒ¼ã‚¹åˆ†ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    log(f"ğŸ ã€{jcd}å ´ {rno}Rã€‘ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    url_before = f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}"
    url_res = f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}"
    url_list = f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}"
    
    # ãƒšãƒ¼ã‚¸å–å¾—ï¼ˆå¤±æ•—ã—ãŸã‚‰Noneã§å³çµ‚äº†ï¼‰
    soup_before = get_soup(url_before, "ç›´å‰æƒ…å ±")
    if not soup_before: return None

    soup_res = get_soup(url_res, "ãƒ¬ãƒ¼ã‚¹çµæœ")
    if not soup_res: return None

    soup_list = get_soup(url_list, "ç•ªçµ„è¡¨")
    if not soup_list: return None

    try:
        log(f"  -> ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºãƒ»è§£æä¸­...")
        row = {'date': date_str, 'jcd': jcd, 'rno': rno}

        # --- â‘  é¢¨é€Ÿ ---
        try:
            wind_elem = soup_before.select_one(".weather1_bodyUnitLabelData")
            row['wind'] = float(clean_text(wind_elem.text).replace("m", "")) if wind_elem else 0.0
        except: row['wind'] = 0.0

        # --- â‘¡ ç€é † (rank1, rank2, rank3) ---
        row['rank1'], row['rank2'], row['rank3'] = None, None, None
        try:
            rank_rows = soup_res.select("table.is-w495 tbody tr")
            for r in rank_rows:
                tds = r.select("td")
                if len(tds) > 1:
                    rank_idx = clean_text(tds[0].text)
                    boat_num = clean_text(tds[1].text)
                    if rank_idx.isdigit() and int(rank_idx) <= 3:
                        row[f'rank{rank_idx}'] = int(boat_num)
        except: pass
        
        row['res1'] = 1 if row.get('rank1') == 1 else 0

        # --- â‘¢ 3é€£å˜é…å½“ (payout) ---
        row['payout'] = 0
        try:
            # "3é€£å˜" ã‚’å«ã‚€ th ã‚’æ¢ã™
            payout_th = soup_res.find(lambda tag: tag.name == "th" and "3é€£å˜" in tag.text)
            if payout_th:
                # è¦ªã® tr ã‚’å–å¾—ã—ã€ãã®ä¸­ã® td ã‚’æ¢ã™ï¼ˆã‚ˆã‚Šç¢ºå®Ÿãªæ–¹æ³•ï¼‰
                parent_tr = payout_th.find_parent("tr")
                tds = parent_tr.select("td")
                # é€šå¸¸: [0]=çµ„ç•ª, [1]=æ‰•æˆ»é‡‘, [2]=äººæ°—
                if len(tds) >= 2:
                    val_text = clean_text(tds[1].text)
                    if val_text.isdigit():
                        row['payout'] = int(val_text)
                    else:
                        log(f"     âš ï¸ é…å½“è§£æå¤±æ•—: '{val_text}'")
                else:
                    log("     âš ï¸ é…å½“ã®åˆ—(td)ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            else:
                log("     âš ï¸ '3é€£å˜'ã®ãƒ˜ãƒƒãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        except Exception as e:
            log(f"     âš ï¸ é…å½“å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        # --- â‘£ å„è‰‡ãƒ‡ãƒ¼ã‚¿ ---
        for i in range(1, 7):
            try:
                # å±•ç¤ºã‚¿ã‚¤ãƒ 
                boat_cell = soup_before.select_one(f".is-boatColor{i}")
                if boat_cell:
                    tds = boat_cell.find_parent("tbody").select("td")
                    ex_val = clean_text(tds[4].text)
                    row[f'ex{i}'] = float(ex_val) if ex_val and ex_val != "." else 0.0
                else: row[f'ex{i}'] = 0.0

                # è©³ç´°ãƒ‡ãƒ¼ã‚¿
                list_tbody = soup_list.select_one(f".is-boatColor{i}").find_parent("tbody")
                tds = list_tbody.select("td")
                
                wr_match = re.search(r"(\d\.\d{2})", clean_text(tds[3].text))
                row[f'wr{i}'] = float(wr_match.group(1)) if wr_match else 0.0
                
                f_match = re.search(r"F(\d+)", clean_text(tds[2].text))
                row[f'f{i}'] = int(f_match.group(1)) if f_match else 0
                
                st_match = re.search(r"ST(\d\.\d{2})", list_tbody.text.replace("\n", ""))
                row[f'st{i}'] = float(st_match.group(1)) if st_match else 0.17
                
                mo_text = clean_text(tds[5].text)
                mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                if not mo_match:
                    mo_text = clean_text(tds[6].text)
                    mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                row[f'mo{i}'] = float(mo_match.group(1)) if mo_match else 0.0
                
            except:
                row[f'wr{i}'], row[f'f{i}'], row[f'st{i}'], row[f'mo{i}'] = 0.0, 0, 0.20, 0.0

        # æˆåŠŸãƒ­ã‚°
        log(f"  âœ… å–å¾—æˆåŠŸ (1ç€:{row.get('rank1')} / é…å½“:Â¥{row.get('payout')})")
        return row

    except Exception as e:
        log(f"  âŒ ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼: {e}")
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", default=DEFAULT_TARGET_DATE)
    parser.add_argument("--end", default=None)
    args = parser.parse_args()

    target_date = args.start.replace("-", "")
    log(f"ğŸš€ DEBUG START: {target_date} (Max: {MAX_RACES} races)")
    log("==================================================")
    
    collected_data = []
    
    # 24å ´Ã—12Rã‚’å·¡å›
    for jcd in range(1, 25):
        if len(collected_data) >= MAX_RACES: break
        
        for rno in range(1, 13):
            if len(collected_data) >= MAX_RACES: break
            
            data = scrape_race(jcd, rno, target_date)
            if data:
                collected_data.append(data)
                # é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ã‚’é˜²ããŸã‚å°‘ã—é•·ã‚ã«å¾…ã¤
                time.sleep(3) 
            
    # CSVä¿å­˜
    if collected_data:
        log("==================================================")
        os.makedirs("data", exist_ok=True)
        df = pd.DataFrame(collected_data)
        
        cols = ['date', 'jcd', 'rno', 'wind', 'res1', 'rank1', 'rank2', 'rank3', 'payout']
        for i in range(1, 7):
            cols.extend([f'wr{i}', f'mo{i}', f'ex{i}', f'f{i}', f'st{i}'])
        
        use_cols = [c for c in cols if c in df.columns]
        df = df[use_cols]

        output_path = "data/debug_result.csv"
        df.to_csv(output_path, index=False)
        log(f"ğŸ‰ å®Œäº†ï¼ {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    else:
        log("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒ1ä»¶ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(1)
