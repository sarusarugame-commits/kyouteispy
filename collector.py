import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import re
import os
import unicodedata
import sys
import argparse
import random

# ==========================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
DEFAULT_TARGET_DATE = "20250101" 
MAX_RACES = 5 

UA_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
]

def log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def clean_text(text):
    if not text: return ""
    text = unicodedata.normalize('NFKC', str(text))
    return text.replace("\n", "").replace("\r", "").replace("Â¥", "").replace(",", "").strip()

def get_soup(url, description="ãƒšãƒ¼ã‚¸"):
    for i in range(1, 4):
        try:
            headers = {'User-Agent': random.choice(UA_LIST)}
            res = requests.get(url, headers=headers, timeout=30)
            res.encoding = res.apparent_encoding
            if res.status_code == 200:
                if "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in res.text: return None
                return BeautifulSoup(res.text, 'html.parser')
            time.sleep(3)
        except: time.sleep(3)
    return None

def extract_payout(soup, key_text):
    """æŒ‡å®šã—ãŸè³­ã‘å¼ï¼ˆå˜å‹ã€2é€£å˜ãªã©ï¼‰ã®é…å½“ã‚’æŠ½å‡ºã™ã‚‹"""
    try:
        # ãã®æ–‡å­—ã‚’å«ã‚€è¡Œã‚’æ¢ã™
        target_th = soup.find(lambda tag: tag.name == "th" and key_text in tag.text)
        if target_th:
            # thã®è¦ªtrã‚’å–å¾— -> ãã®ä¸­ã®tdã‚’æ¢ã™
            parent_tr = target_th.find_parent("tr")
            tds = parent_tr.select("td")
            
            # é…å½“é‡‘ã£ã½ã„æ•°å­—ã‚’æ¢ã™ï¼ˆçµ„ç•ªã®æ¬¡ã«ã‚ã‚‹ã“ã¨ãŒå¤šã„ï¼‰
            for td in tds:
                txt = clean_text(td.text)
                # æ•°å­—ã®ã¿ã§ã€ã‹ã¤ "-" ã‚’å«ã¾ãªã„ï¼ˆçµ„ç•ªã§ã¯ãªã„ï¼‰ã‚‚ã®ã‚’é…å½“ã¨ã¿ãªã™
                if txt.isdigit() and len(txt) > 1 and "-" not in txt:
                    return int(txt)
    except: pass
    return 0

def scrape_race(jcd, rno, date_str):
    log(f"ğŸ ã€{jcd}å ´ {rno}Rã€‘ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    soup_before = get_soup(f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}", "ç›´å‰æƒ…å ±")
    if not soup_before: return None

    soup_res = get_soup(f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}", "ãƒ¬ãƒ¼ã‚¹çµæœ")
    if not soup_res: return None

    soup_list = get_soup(f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}", "ç•ªçµ„è¡¨")
    if not soup_list: return None

    try:
        row = {'date': date_str, 'jcd': jcd, 'rno': rno}

        # --- â‘  é¢¨é€Ÿ ---
        try:
            wind_elem = soup_before.select_one(".weather1_bodyUnitLabelData")
            row['wind'] = float(clean_text(wind_elem.text).replace("m", "").replace(" ", "")) if wind_elem else 0.0
        except: row['wind'] = 0.0

        # --- â‘¡ ç€é † ---
        row['rank1'], row['rank2'], row['rank3'] = None, None, None
        try:
            rank_rows = soup_res.select("table.is-w495 tbody tr")
            for r in rank_rows:
                tds = r.select("td")
                if len(tds) > 1:
                    rank_idx = clean_text(tds[0].text).replace(" ", "")
                    boat_text = clean_text(tds[1].text)
                    boat_match = re.search(r"^(\d{1})", boat_text)
                    if rank_idx.isdigit() and int(rank_idx) <= 3 and boat_match:
                        row[f'rank{rank_idx}'] = int(boat_match.group(1))
        except: pass
        row['res1'] = 1 if row.get('rank1') == 1 else 0

        # --- â‘¢ é…å½“ï¼ˆã“ã“ã‚’å¼·åŒ–ã—ã¾ã—ãŸï¼‰ ---
        # å¿…è¦ãªè³­ã‘å¼ã‚’å…¨éƒ¨å–ã‚‹
        row['tansho'] = extract_payout(soup_res, "å˜å‹")
        row['nirentan'] = extract_payout(soup_res, "2é€£å˜")
        row['sanrentan'] = extract_payout(soup_res, "3é€£å˜")
        row['sanrenpuku'] = extract_payout(soup_res, "3é€£è¤‡")
        
        # äº’æ›æ€§ã®ãŸã‚ payout = 3é€£å˜ ã«ã—ã¦ãŠã
        row['payout'] = row['sanrentan']

        # --- â‘£ å„è‰‡ãƒ‡ãƒ¼ã‚¿ ---
        for i in range(1, 7):
            try:
                # å±•ç¤º
                boat_cell = soup_before.select_one(f".is-boatColor{i}")
                if boat_cell:
                    tds = boat_cell.find_parent("tbody").select("td")
                    ex_val = clean_text(tds[4].text).replace(" ", "")
                    row[f'ex{i}'] = float(ex_val) if ex_val and ex_val != "." else 0.0
                else: row[f'ex{i}'] = 0.0

                # è©³ç´°
                list_tbody = soup_list.select_one(f".is-boatColor{i}").find_parent("tbody")
                tds = list_tbody.select("td")
                
                wr_match = re.search(r"(\d\.\d{2})", clean_text(tds[3].text))
                row[f'wr{i}'] = float(wr_match.group(1)) if wr_match else 0.0
                
                f_match = re.search(r"F(\d+)", clean_text(tds[2].text))
                row[f'f{i}'] = int(f_match.group(1)) if f_match else 0
                
                st_match = re.search(r"ST(\d\.\d{2})", list_tbody.text.replace("\n", "").replace(" ", ""))
                row[f'st{i}'] = float(st_match.group(1)) if st_match else 0.17
                
                mo_text = clean_text(tds[5].text)
                mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                if not mo_match:
                    mo_text = clean_text(tds[6].text)
                    mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                row[f'mo{i}'] = float(mo_match.group(1)) if mo_match else 0.0
            except:
                row[f'wr{i}'], row[f'f{i}'], row[f'st{i}'], row[f'mo{i}'] = 0.0, 0, 0.20, 0.0

        log(f"  âœ… å–å¾—æˆåŠŸ (å˜:{row['tansho']} / 2é€£:{row['nirentan']} / 3é€£:{row['sanrentan']})")
        return row

    except Exception as e:
        log(f"  âŒ ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼: {e}")
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", default=DEFAULT_TARGET_DATE)
    parser.add_argument("--end", default=None)
    args = parser.parse_args()

    target_date = args.start.replace("-", "")
    log(f"ğŸš€ DEBUG START: {target_date} (Max: {MAX_RACES} races)")
    
    collected_data = []
    
    for jcd in range(1, 25):
        if len(collected_data) >= MAX_RACES: break
        for rno in range(1, 13):
            if len(collected_data) >= MAX_RACES: break
            data = scrape_race(jcd, rno, target_date)
            if data:
                collected_data.append(data)
                time.sleep(3) 
            
    if collected_data:
        os.makedirs("data", exist_ok=True)
        df = pd.DataFrame(collected_data)
        
        # ã‚«ãƒ©ãƒ é †åºæ•´ç†ï¼ˆé…å½“ç³»ã‚’å‰ã«ï¼‰
        cols = ['date', 'jcd', 'rno', 'wind', 'res1', 'rank1', 'rank2', 'rank3', 
                'tansho', 'nirentan', 'sanrentan', 'sanrenpuku', 'payout']
        for i in range(1, 7):
            cols.extend([f'wr{i}', f'mo{i}', f'ex{i}', f'f{i}', f'st{i}'])
        
        use_cols = [c for c in cols if c in df.columns]
        df = df[use_cols]

        output_path = "data/debug_result.csv"
        df.to_csv(output_path, index=False)
        log(f"ğŸ‰ å®Œäº†ï¼ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    else:
        sys.exit(1)
