import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import sys
import argparse
import os
import re
from datetime import datetime, timedelta # ğŸ‘ˆ ã“ã‚Œã§ã‚¨ãƒ©ãƒ¼ã¯æ¶ˆãˆã¾ã™

# ãƒ­ã‚°ã‚’å³æ™‚è¡¨ç¤º
sys.stdout.reconfigure(line_buffering=True)

# ==========================================
# âš™ï¸ è¨­å®š
# ==========================================
MAX_RETRIES = 3
RETRY_INTERVAL = 3

def get_session():
    session = requests.Session()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    }
    session.headers.update(headers)
    return session

def get_soup_with_retry(session, url):
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            res = session.get(url, timeout=30)
            res.encoding = res.apparent_encoding
            if res.status_code == 200:
                return BeautifulSoup(res.text, 'html.parser')
        except:
            pass
        time.sleep(RETRY_INTERVAL)
    return None

def clean_text(text):
    if not text: return ""
    return text.replace("\n", "").replace("\r", "").replace(" ", "").replace("\u3000", "").strip()

def scrape_race_data(session, jcd, rno, date_str):
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    print(f"ğŸ” {date_str} {jcd}å ´ {rno}R: ", end="")
    
    # 3ãƒšãƒ¼ã‚¸å–å¾—
    soup_list = get_soup_with_retry(session, f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    soup_before = get_soup_with_retry(session, f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    soup_res = get_soup_with_retry(session, f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}")

    if not all([soup_list, soup_before, soup_res]):
        print("âŒ HTMLå–å¾—å¤±æ•—")
        return None

    try:
        # --- 1. é¢¨é€Ÿå–å¾— ---
        wind = 0.0
        try:
            wind_elem = soup_before.find(string=re.compile("é¢¨é€Ÿ"))
            if wind_elem:
                parent = wind_elem.find_parent(class_="weather1_bodyUnit")
                if parent:
                    data_elem = parent.select_one(".weather1_bodyUnitLabelData")
                    if data_elem:
                        w_text = clean_text(data_elem.text).replace("m", "")
                        wind = float(w_text)
        except:
            pass 

        # --- 2. æ­£è§£ãƒ©ãƒ™ãƒ« (1ç€) ---
        res1 = 0
        try:
            res_rows = soup_res.select(".is-p_1-1")
            if res_rows:
                rank1_boat = clean_text(res_rows[0].select("td")[1].text)
                if rank1_boat == "1":
                    res1 = 1
        except:
            pass

        # --- 3. å±•ç¤ºã‚¿ã‚¤ãƒ  & å„è‰‡ãƒ‡ãƒ¼ã‚¿ ---
        temp_ex_times = []
        
        for i in range(1, 7):
            # è‰‡ç•ªã®è‰²ã‚¯ãƒ©ã‚¹ã‹ã‚‰æ¢ã™ç¢ºå®Ÿãªæ–¹æ³•
            boat_cell = soup_before.select_one(f".is-boatColor{i}")
            if not boat_cell:
                print(f"âš ï¸ {i}å·è‰‡ãªã— ", end="")
                return None
            
            tbody = boat_cell.find_parent("tbody")
            tds = tbody.select("td")
            
            # [å†™çœŸ, é¸æ‰‹å, ä½“é‡, å±•ç¤º, ãƒãƒ«ãƒˆ...] -> é€šå¸¸ã¯index 4
            ex_val = clean_text(tds[4].text)
            if not ex_val: ex_val = clean_text(tds[5].text) # ã‚ºãƒ¬å¯¾ç­–
            
            if not ex_val or ex_val == "-" or ex_val == "0.00":
                print(f"âš ï¸ {i}å·è‰‡å±•ç¤ºæ¬ æ ", end="")
                return None
            
            try:
                temp_ex_times.append(float(ex_val))
            except:
                print(f"âŒ æ•°å€¤åŒ–ä¸å¯[{ex_val}] ", end="")
                return None

        # --- 4. å‡ºèµ°è¡¨ãƒ‡ãƒ¼ã‚¿ ---
        row = {'date': date_str, 'jcd': jcd, 'rno': rno, 'wind': wind, 'res1': res1}
        
        for i in range(1, 7):
            boat_cell_list = soup_list.select_one(f".is-boatColor{i}")
            if not boat_cell_list: return None
            
            tbody_list = boat_cell_list.find_parent("tbody")
            tds_list = tbody_list.select("td")
            
            try:
                # å…¨å›½å‹ç‡
                row[f'wr{i}'] = float(re.findall(r"\d+\.\d+", tds_list[3].text)[0])
                # ãƒ¢ãƒ¼ã‚¿ãƒ¼2é€£ç‡
                nums = re.findall(r"\d+\.\d+", tds_list[6].text)
                if len(nums) >= 1:
                     row[f'mo{i}'] = float(nums[0])
                else:
                     row[f'mo{i}'] = 0.0
            except:
                row[f'wr{i}'] = 0.0
                row[f'mo{i}'] = 0.0

            row[f'ex{i}'] = temp_ex_times[i-1]

        print("âœ… OK")
        return row

    except Exception as e:
        print(f"ğŸ’¥ {e}")
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", required=True)
    parser.add_argument("--end", required=True)
    args = parser.parse_args()

    os.makedirs("data", exist_ok=True)
    session = get_session()
    
    # èªè¨¼
    try:
        session.get("https://www.boatrace.jp/", timeout=10)
    except:
        pass

    # ğŸ”¥ 1ãƒ¬ãƒ¼ã‚¹é™å®šãƒ‡ãƒãƒƒã‚° ğŸ”¥
    # 1æœˆ1æ—¥ã€æ¡ç”Ÿ(01)ã€1R å›ºå®š
    d_str = "20250101"
    jcd = 1
    rno = 1
    
    print(f"ğŸš€ 1ãƒ¬ãƒ¼ã‚¹é™å®šãƒ‡ãƒãƒƒã‚°é–‹å§‹")
    
    results = []
    data = scrape_race_data(session, jcd, rno, d_str)
    
    if data:
        results.append(data)
        df = pd.DataFrame(results)
        filename = f"data/pure_data_debug_1R.csv"
        df.to_csv(filename, index=False)
        print(f"\nğŸ‰ å®Œäº†ï¼CSVä¿å­˜ã—ã¾ã—ãŸ: {filename}")
        print(df) # ãƒ­ã‚°ã«ä¸­èº«ã‚’è¡¨ç¤º
    else:
        print("\nğŸ’€ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
