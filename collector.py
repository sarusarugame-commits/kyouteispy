import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import sys
import argparse
import os

# ==========================================
# âš™ï¸ è¨­å®šï¼ˆã‚¹ãƒ†ãƒ«ã‚¹ãƒ¢ãƒ¼ãƒ‰ï¼‰
# ==========================================
MAX_RETRIES = 3
RETRY_INTERVAL = 5 # ç„¦ã‚‰ãš5ç§’å¾…ã¤

def get_session():
    """äººé–“ã‚‰ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    session = requests.Session()
    # ä¸€èˆ¬çš„ãªWindowsã®Chromeã«è¦‹ã›ã‹ã‘ã‚‹å¼·åŠ›ãªå½è£…ãƒ˜ãƒƒãƒ€ãƒ¼
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': 'https://www.boatrace.jp/',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'same-origin',
        'Sec-Fetch-User': '?1'
    }
    session.headers.update(headers)
    return session

def get_soup_with_retry(session, url):
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ã£ã¦ã‚¢ã‚¯ã‚»ã‚¹"""
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            print(f"ğŸŒ ã‚¢ã‚¯ã‚»ã‚¹ä¸­ ({attempt}/{MAX_RETRIES}): {url}", flush=True)
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’30ç§’ã«å»¶é•·ï¼ˆç²˜ã‚‹ï¼‰
            res = session.get(url, timeout=30)
            res.encoding = res.apparent_encoding
            
            if res.status_code == 200:
                return BeautifulSoup(res.text, 'html.parser')
            elif res.status_code == 403:
                print("â›” 403 Forbidden: ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦ã•ã‚Œã¾ã—ãŸï¼ˆIPãƒ–ãƒ­ãƒƒã‚¯ã®å¯èƒ½æ€§å¤§ï¼‰", flush=True)
            else:
                print(f"âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {res.status_code}", flush=True)
                
        except Exception as e:
            print(f"ğŸ’¥ ã‚¨ãƒ©ãƒ¼: {e}", flush=True)
        
        if attempt < MAX_RETRIES:
            print(f"ğŸ’¤ {RETRY_INTERVAL}ç§’ å¾…æ©Ÿ...", flush=True)
            time.sleep(RETRY_INTERVAL)
            
    return None

def scrape_race_data(session, jcd, rno, date_str):
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    # 1ãƒ¬ãƒ¼ã‚¹å†…ã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆCookieï¼‰ã‚’ä½¿ã„å›ã™
    soup_list = get_soup_with_retry(session, f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    if not soup_list: return None
    # å°‘ã—é–“éš”ã‚’ç©ºã‘ã‚‹ï¼ˆäººé–“ã‚¢ãƒ”ãƒ¼ãƒ«ï¼‰
    time.sleep(1)
    
    soup_before = get_soup_with_retry(session, f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    if not soup_before: return None
    time.sleep(1)

    soup_res = get_soup_with_retry(session, f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    if not soup_res: return None

    try:
        w_text = soup_before.select_one('.weather1_bodyUnitLabelData').text.replace('m','').strip()
        wind = float(w_text) if w_text else 0.0

        res1_text = soup_res.select_one('.is-p_0-1 .is-p_1-1') 
        res1 = 1 if (res1_text and res1_text.text.strip() == "1") else 0

        temp_ex_times = []
        for i in range(1, 7):
            ex_val = soup_before.select(f'tbody.is-p_0-{i}')[0].select('td')[4].text.strip()
            if not ex_val or ex_val == "-" or float(ex_val) <= 0:
                return None
            temp_ex_times.append(float(ex_val))

        row = {'date': date_str, 'jcd': jcd, 'rno': rno, 'wind': wind, 'res1': res1}
        for i in range(1, 7):
            tbody = soup_list.select(f'tbody.is-p_0-{i}')[0].select('td')
            row[f'wr{i}'] = float(tbody[3].select_one('div').text.split()[0])
            row[f'mo{i}'] = float(tbody[6].select_one('div').text.split()[0])
            row[f'ex{i}'] = temp_ex_times[i-1]

        return row
    except:
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", required=True)
    parser.add_argument("--end", required=True)
    args = parser.parse_args()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹
    session = get_session()
    
    # ã¾ãšãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦Cookieã‚’ã‚‚ã‚‰ã†ï¼ˆé‡è¦ï¼ï¼‰
    print("ğŸ  ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã«æŒ¨æ‹¶ä¸­...", flush=True)
    get_soup_with_retry(session, "https://www.boatrace.jp/")

    start_d = datetime.strptime(args.start, "%Y-%m-%d")
    end_d = datetime.strptime(args.end, "%Y-%m-%d")
    current = start_d
    results = []

    # ãƒ†ã‚¹ãƒˆã®ãŸã‚ã€ã¾ãšã¯ã€Œæœ€åˆã®1ä¼šå ´ãƒ»1ãƒ¬ãƒ¼ã‚¹ã€ã ã‘è©¦ã™å®‰å…¨è£…ç½®
    # ã†ã¾ãã„ã£ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã«æˆ»ã™
    d_str = current.strftime("%Y%m%d")
    print(f"ğŸš€ ãƒ†ã‚¹ãƒˆåé›†: {d_str} ä¼šå ´01 ãƒ¬ãƒ¼ã‚¹01", flush=True)
    
    data = scrape_race_data(session, 1, 1, d_str)
    if data:
        print("âœ… çªç ´æˆåŠŸï¼ãƒ‡ãƒ¼ã‚¿ãŒå–ã‚Œã¾ã—ãŸï¼", flush=True)
        print(data, flush=True)
    else:
        print("âŒ çªç ´å¤±æ•—ã€‚ã‚„ã¯ã‚ŠIPãƒ–ãƒ­ãƒƒã‚¯ãŒå¼·åŠ›ã§ã™ã€‚", flush=True)
