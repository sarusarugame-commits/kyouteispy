import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import re
import os
import unicodedata
import sys
import argparse
import random
import threading
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

# ==========================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
MAX_WORKERS = 16       # 16ä¸¦åˆ—ã§æ”»ã‚ã‚‹
MAX_RETRIES = 5        # ç²˜ã‚Šå¼·ããƒªãƒˆãƒ©ã‚¤
RETRY_DELAY = 3        # ãƒªãƒˆãƒ©ã‚¤æ™‚ã®å¾…æ©Ÿç§’æ•°

# å½è£…ç”¨User-Agent
UA_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
]

# ãƒ­ã‚°æŽ’ä»–åˆ¶å¾¡ç”¨
print_lock = threading.Lock()

def safe_print(msg):
    with print_lock:
        print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def clean_text(text):
    if not text: return ""
    text = unicodedata.normalize('NFKC', str(text))
    return text.replace("\n", "").replace("\r", "").replace("Â¥", "").replace(",", "").strip()

def get_session():
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã‚»ãƒƒã‚·ãƒ§ãƒ³"""
    session = requests.Session()
    retries = Retry(total=MAX_RETRIES, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(pool_connections=MAX_WORKERS, pool_maxsize=MAX_WORKERS, max_retries=retries)
    session.mount("https://", adapter)
    return session

def get_soup(session, url, description="ãƒšãƒ¼ã‚¸"):
    for i in range(MAX_RETRIES):
        try:
            headers = {'User-Agent': random.choice(UA_LIST)}
            res = session.get(url, headers=headers, timeout=30)
            res.encoding = res.apparent_encoding
            
            if res.status_code == 200:
                if "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in res.text: return None, "SKIP"
                return BeautifulSoup(res.text, 'html.parser'), None
            time.sleep(random.uniform(1, 3))
        except:
            time.sleep(RETRY_DELAY)
    return None, "ERROR"

def extract_payout(soup, key_text):
    """å¼·åŠ›ãªé…å½“æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯"""
    try:
        tables = soup.select("table")
        for tbl in tables:
            if key_text in tbl.text:
                rows = tbl.select("tr")
                for tr in rows:
                    if key_text in tr.text:
                        tds = tr.select("td")
                        for td in tds:
                            txt = clean_text(td.text)
                            if txt.isdigit() and len(txt) >= 2 and "-" not in txt:
                                return int(txt)
    except: pass
    return 0

def scrape_race_data(session, jcd, rno, date_str):
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    # 3ãƒšãƒ¼ã‚¸å–å¾—ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è€ƒæ…®ã—ã¤ã¤ç¢ºå®Ÿã«ï¼‰
    soup_before, err = get_soup(session, f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}", "ç›´å‰")
    if err == "SKIP" or not soup_before: return None

    soup_res, err = get_soup(session, f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}", "çµæžœ")
    if not soup_res: return None

    soup_list, err = get_soup(session, f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}", "ç•ªçµ„")
    if not soup_list: return None

    try:
        row = {'date': date_str, 'jcd': jcd, 'rno': rno}

        # --- â‘  é¢¨é€Ÿ ---
        try:
            wind_elem = soup_before.select_one(".weather1_bodyUnitLabelData")
            row['wind'] = float(clean_text(wind_elem.text).replace("m", "").replace(" ", "")) if wind_elem else 0.0
        except: row['wind'] = 0.0

        # --- â‘¡ ç€é † ---
        row['rank1'], row['rank2'], row['rank3'] = None, None, None
        try:
            rank_rows = soup_res.select("table.is-w495 tbody tr")
            for r in rank_rows:
                tds = r.select("td")
                if len(tds) > 1:
                    rank_idx = clean_text(tds[0].text).replace(" ", "")
                    boat_text = clean_text(tds[1].text)
                    boat_match = re.search(r"^(\d{1})", boat_text)
                    if rank_idx.isdigit() and int(rank_idx) <= 3 and boat_match:
                        row[f'rank{rank_idx}'] = int(boat_match.group(1))
        except: pass
        row['res1'] = 1 if row.get('rank1') == 1 else 0

        # --- â‘¢ é…å½“ï¼ˆ3é€£å˜ã‚’payoutã¨ã—ã¦ä½¿ç”¨ï¼‰ ---
        row['tansho'] = extract_payout(soup_res, "å˜å‹")
        row['nirentan'] = extract_payout(soup_res, "2é€£å˜")
        row['sanrentan'] = extract_payout(soup_res, "3é€£å˜")
        row['sanrenpuku'] = extract_payout(soup_res, "3é€£è¤‡")
        row['payout'] = row['sanrentan'] 

        # --- â‘£ å„è‰‡ãƒ‡ãƒ¼ã‚¿ ---
        for i in range(1, 7):
            # -------------------------------------------------------
            # [A] ç›´å‰æƒ…å ± (beforeinfo) ã‹ã‚‰å–å¾—: å±•ç¤ºã‚¿ã‚¤ãƒ , ãƒ¢ãƒ¼ã‚¿ãƒ¼å‹çŽ‡
            # -------------------------------------------------------
            try:
                boat_cell = soup_before.select_one(f".is-boatColor{i}")
                if boat_cell:
                    tbody = boat_cell.find_parent("tbody")
                    tds = tbody.select("td")
                    
                    # å±•ç¤ºã‚¿ã‚¤ãƒ  (é€šå¸¸ã¯å³ç«¯ã®æ–¹ã«ã‚ã‚‹)
                    # tdã®ä¸­èº«ã‚’èµ°æŸ»ã—ã¦ "6.xx" ã®ã‚ˆã†ãªå½¢å¼ã‚’æŽ¢ã™æ–¹ãŒå®‰å…¨ã ãŒã€é…ç½®å›ºå®šã¨ä»®å®š
                    ex_val = clean_text(tds[-1].text).replace(" ", "") # ä¸€ç•ªå³
                    if not re.match(r"\d\.\d{2}", ex_val):
                         ex_val = clean_text(tds[4].text).replace(" ", "") # å¿µã®ãŸã‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æŒ‡å®šã‚‚è©¦è¡Œ
                    row[f'ex{i}'] = float(ex_val) if ex_val and ex_val != "." else 0.0

                    # ãƒ¢ãƒ¼ã‚¿ãƒ¼å‹çŽ‡ (2é€£çŽ‡)
                    # "No.xx xx.x%" ã¨ã„ã†å½¢å¼ã®ã‚»ãƒ«ã‚’æŽ¢ã™
                    row[f'mo{i}'] = 0.0
                    for td in tds:
                        txt = clean_text(td.text)
                        # "%" ãŒå«ã¾ã‚Œã¦ã„ã¦æ•°å­—ãŒã‚ã‚‹å ´åˆ
                        if "%" in txt:
                            mo_match = re.search(r"(\d{1,2}\.\d)", txt)
                            if mo_match:
                                row[f'mo{i}'] = float(mo_match.group(1))
                                break
                else:
                    row[f'ex{i}'] = 0.0
                    row[f'mo{i}'] = 0.0
            except:
                row[f'ex{i}'] = 0.0
                row[f'mo{i}'] = 0.0

            # -------------------------------------------------------
            # [B] ç•ªçµ„è¡¨ (racelist) ã‹ã‚‰å–å¾—: é¸æ‰‹å‹çŽ‡, Fæ•°, ST
            # -------------------------------------------------------
            try:
                list_node = soup_list.select_one(f".is-boatColor{i}")
                if list_node:
                    list_tbody = list_node.find_parent("tbody")
                    row_text = clean_text(list_tbody.text)
                    tds = list_tbody.select("td")
                    
                    # å…¨å›½å‹çŽ‡ (x.xx ã¨ã„ã†å½¢å¼ã‚’æŽ¢ã™)
                    # é€šå¸¸ tds[3] ã‚ãŸã‚Šã ãŒã€è¡Œå…¨ä½“ã‹ã‚‰æ­£è¦è¡¨ç¾ã§æŽ¢ã™
                    wr_match = re.search(r"(\d\.\d{2})", clean_text(tds[3].text))
                    row[f'wr{i}'] = float(wr_match.group(1)) if wr_match else 0.0
                    
                    # ãƒ•ãƒ©ã‚¤ãƒ³ã‚°(F)
                    # è¡Œå…¨ä½“ã‹ã‚‰ "F1", "F2" ãªã©ã‚’æŽ¢ã™
                    f_match = re.search(r"F(\d+)", row_text)
                    row[f'f{i}'] = int(f_match.group(1)) if f_match else 0
                    
                    # å¹³å‡ST
                    # è¡Œå…¨ä½“ã‹ã‚‰ "ST0.15" ã®ã‚ˆã†ãªå½¢å¼ã‚’æŽ¢ã™
                    st_match = re.search(r"ST(\d\.\d{2})", row_text.replace(" ", ""))
                    row[f'st{i}'] = float(st_match.group(1)) if st_match else 0.17
                else:
                    raise Exception("No Data")
                
            except:
                row[f'wr{i}'] = 0.0
                row[f'f{i}'] = 0
                row[f'st{i}'] = 0.17
        
        return row
    except: return None

def process_wrapper(args):
    """ä¸¦åˆ—å®Ÿè¡Œç”¨ãƒ©ãƒƒãƒ‘ãƒ¼"""
    session, jcd, rno, date_str = args
    # å°‘ã—ãƒ©ãƒ³ãƒ€ãƒ ã«å¾…æ©Ÿã—ã¦ã€ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ•£ã•ã›ã‚‹
    time.sleep(random.uniform(0.5, 2.0))
    return scrape_race_data(session, jcd, rno, date_str)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", required=True)
    parser.add_argument("--end", required=True)
    args = parser.parse_args()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«æœ‰åŠ¹åŒ–ï¼‰
    session = get_session()
    
    start_d = datetime.strptime(args.start, "%Y-%m-%d")
    end_d = datetime.strptime(args.end, "%Y-%m-%d")
    current = start_d
    
    safe_print(f"ðŸš€ åŽé›†é–‹å§‹: {args.start} ã€œ {args.end} (ä¸¦åˆ—æ•°: {MAX_WORKERS})")
    
    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    os.makedirs("data", exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
    filename = f"data/data_{args.start.replace('-','')}_{args.end.replace('-','')}.csv"
    file_exists = os.path.exists(filename)

    while current <= end_d:
        d_str = current.strftime("%Y%m%d")
        safe_print(f"ðŸ“… {d_str} å‡¦ç†ä¸­...")
        
        # 1æ—¥åˆ†ã®ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆä½œæˆ
        tasks = []
        for jcd in range(1, 25):
            for rno in range(1, 13):
                tasks.append((session, jcd, rno, d_str))
        
        results = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for res in executor.map(process_wrapper, tasks):
                if res: results.append(res)
        
        # 1æ—¥ã”ã¨ã«ä¿å­˜
        if results:
            df = pd.DataFrame(results)
            # ã‚«ãƒ©ãƒ é †åº
            cols = ['date', 'jcd', 'rno', 'wind', 'res1', 'rank1', 'rank2', 'rank3', 
                    'tansho', 'nirentan', 'sanrentan', 'sanrenpuku', 'payout']
            for i in range(1, 7):
                cols.extend([f'wr{i}', f'mo{i}', f'ex{i}', f'f{i}', f'st{i}'])
            
            # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿ã§æ§‹æˆ
            use_cols = [c for c in cols if c in df.columns]
            df = df[use_cols]
            
            # è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜
            df.to_csv(filename, mode='a', index=False, header=not file_exists)
            file_exists = True
            safe_print(f"  âœ… {len(df)}ãƒ¬ãƒ¼ã‚¹ ä¿å­˜å®Œäº†")
        
        current += timedelta(days=1)
    
    safe_print(f"ðŸŽ‰ å®Œäº†ï¼ãƒ‡ãƒ¼ã‚¿ã¯ {filename} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
