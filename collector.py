import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import re
import os
import unicodedata
import sys
import argparse
import random

# ==========================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
DEFAULT_TARGET_DATE = "20250101" 
MAX_RACES = 5 

# å½è£…ç”¨User-Agentãƒªã‚¹ãƒˆ
UA_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
]

def log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def clean_text(text):
    if not text: return ""
    text = unicodedata.normalize('NFKC', str(text))
    # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤ã™ã‚‹ãŒã€æ•°å­—ãŒãã£ã¤ã‹ãªã„ã‚ˆã†ã«æ³¨æ„
    return text.replace("\n", "").replace("\r", "").replace("Â¥", "").replace(",", "").strip()

def get_soup(url, description="ãƒšãƒ¼ã‚¸"):
    for i in range(1, 4):
        try:
            headers = {'User-Agent': random.choice(UA_LIST)}
            res = requests.get(url, headers=headers, timeout=30)
            res.encoding = res.apparent_encoding
            
            if res.status_code == 200:
                if "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in res.text:
                    log(f"     âš ï¸ {description}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                    return None
                return BeautifulSoup(res.text, 'html.parser')
            time.sleep(3)
        except Exception as e:
            log(f"     âŒ {description}: {e} (Wait 3s...)")
            time.sleep(3)
    log(f"     ğŸ’€ {description}: å–å¾—å¤±æ•—")
    return None

def scrape_race(jcd, rno, date_str):
    log(f"ğŸ ã€{jcd}å ´ {rno}Rã€‘ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    soup_before = get_soup(f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}", "ç›´å‰æƒ…å ±")
    if not soup_before: return None

    soup_res = get_soup(f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}", "ãƒ¬ãƒ¼ã‚¹çµæœ")
    if not soup_res: return None

    soup_list = get_soup(f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}", "ç•ªçµ„è¡¨")
    if not soup_list: return None

    try:
        log(f"  -> ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºãƒ»è§£æä¸­...")
        row = {'date': date_str, 'jcd': jcd, 'rno': rno}

        # --- â‘  é¢¨é€Ÿ ---
        try:
            wind_elem = soup_before.select_one(".weather1_bodyUnitLabelData")
            row['wind'] = float(clean_text(wind_elem.text).replace("m", "").replace(" ", "")) if wind_elem else 0.0
        except: row['wind'] = 0.0

        # --- â‘¡ ç€é † (rank1, rank2, rank3) ---
        # â˜…ä¿®æ­£: è‰‡ç•ªãŒãã£ã¤ãå•é¡Œã«å¯¾å‡¦
        row['rank1'], row['rank2'], row['rank3'] = None, None, None
        try:
            # ç€é †ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—ï¼ˆé€šå¸¸ãƒšãƒ¼ã‚¸ä¸Šéƒ¨ï¼‰
            rank_rows = soup_res.select("table.is-w495 tbody tr")
            for r in rank_rows:
                tds = r.select("td")
                if len(tds) > 1:
                    # ç€é †
                    rank_idx = clean_text(tds[0].text).replace(" ", "")
                    
                    # è‰‡ç•ªï¼ˆã“ã“ãŒé‡è¦ï¼šæ•°å­—ã ã‘ã‚’å³å¯†ã«æŠ½å‡ºï¼‰
                    boat_text = clean_text(tds[1].text)
                    boat_match = re.search(r"^(\d{1})", boat_text) # å…ˆé ­ã®1æ¡ã ã‘ã‚’å–ã‚‹
                    
                    if rank_idx.isdigit() and int(rank_idx) <= 3 and boat_match:
                        row[f'rank{rank_idx}'] = int(boat_match.group(1))
        except Exception as e:
            log(f"     âš ï¸ ç€é †è§£æã‚¨ãƒ©ãƒ¼: {e}")

        row['res1'] = 1 if row.get('rank1') == 1 else 0

        # --- â‘¢ 3é€£å˜é…å½“ (payout) ---
        # â˜…ä¿®æ­£: æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ã‚’æŸ”è»Ÿã«
        row['payout'] = 0
        try:
            # ãƒšãƒ¼ã‚¸å†…ã®å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’èµ°æŸ»ã—ã¦ã€Œ3é€£å˜ã€ã‚’æ¢ã™
            found_payout = False
            tables = soup_res.select("table")
            for tbl in tables:
                if "3é€£å˜" in tbl.text:
                    # ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¸­ã«é…å½“ãŒã‚ã‚‹ã¯ãš
                    rows = tbl.select("tr")
                    for tr in rows:
                        # ãƒ˜ãƒƒãƒ€ãƒ¼(th)ã‹ã‚»ãƒ«(td)ã« "3é€£å˜" ãŒã‚ã‚‹è¡Œã‚’æ¢ã™
                        if "3é€£å˜" in tr.text:
                            tds = tr.select("td")
                            # [çµ„ç•ª, æ‰•æˆ»é‡‘, äººæ°—] ã®ä¸¦ã³ãŒå¤šã„ãŒã€å ´åˆã«ã‚ˆã‚‹ã®ã§é‡‘é¡ã£ã½ã„ã‚‚ã®ã‚’æ¢ã™
                            for td in tds:
                                txt = clean_text(td.text)
                                # æ•°å­—ã®ã¿ï¼ˆé‡‘é¡ï¼‰ã§ã€çµ„ç•ªï¼ˆ1-2-3ãªã©ï¼‰ã§ã¯ãªã„ã‚‚ã®ã‚’æ¢ã™
                                if txt.isdigit() and len(txt) > 1 and "-" not in txt:
                                    row['payout'] = int(txt)
                                    found_payout = True
                                    break
                        if found_payout: break
                if found_payout: break
            
            if not found_payout:
                 log("     âš ï¸ é…å½“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        except Exception as e:
            log(f"     âš ï¸ é…å½“å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        # --- â‘£ å„è‰‡ãƒ‡ãƒ¼ã‚¿ ---
        for i in range(1, 7):
            try:
                # å±•ç¤ºã‚¿ã‚¤ãƒ 
                boat_cell = soup_before.select_one(f".is-boatColor{i}")
                if boat_cell:
                    tds = boat_cell.find_parent("tbody").select("td")
                    ex_val = clean_text(tds[4].text).replace(" ", "")
                    row[f'ex{i}'] = float(ex_val) if ex_val and ex_val != "." else 0.0
                else: row[f'ex{i}'] = 0.0

                # è©³ç´°ãƒ‡ãƒ¼ã‚¿
                list_tbody = soup_list.select_one(f".is-boatColor{i}").find_parent("tbody")
                tds = list_tbody.select("td")
                
                wr_match = re.search(r"(\d\.\d{2})", clean_text(tds[3].text))
                row[f'wr{i}'] = float(wr_match.group(1)) if wr_match else 0.0
                
                f_match = re.search(r"F(\d+)", clean_text(tds[2].text))
                row[f'f{i}'] = int(f_match.group(1)) if f_match else 0
                
                # å¹³å‡ST
                st_match = re.search(r"ST(\d\.\d{2})", list_tbody.text.replace("\n", "").replace(" ", ""))
                row[f'st{i}'] = float(st_match.group(1)) if st_match else 0.17
                
                # ãƒ¢ãƒ¼ã‚¿ãƒ¼
                mo_text = clean_text(tds[5].text)
                mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                if not mo_match:
                    mo_text = clean_text(tds[6].text)
                    mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                row[f'mo{i}'] = float(mo_match.group(1)) if mo_match else 0.0
                
            except:
                row[f'wr{i}'], row[f'f{i}'], row[f'st{i}'], row[f'mo{i}'] = 0.0, 0, 0.20, 0.0

        log(f"  âœ… å–å¾—æˆåŠŸ (1ç€:{row.get('rank1')} / é…å½“:Â¥{row.get('payout')})")
        return row

    except Exception as e:
        log(f"  âŒ ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼: {e}")
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", default=DEFAULT_TARGET_DATE)
    parser.add_argument("--end", default=None)
    args = parser.parse_args()

    target_date = args.start.replace("-", "")
    log(f"ğŸš€ DEBUG START: {target_date} (Max: {MAX_RACES} races)")
    log("==================================================")
    
    collected_data = []
    
    for jcd in range(1, 25):
        if len(collected_data) >= MAX_RACES: break
        
        for rno in range(1, 13):
            if len(collected_data) >= MAX_RACES: break
            
            data = scrape_race(jcd, rno, target_date)
            if data:
                collected_data.append(data)
                time.sleep(3) 
            
    if collected_data:
        log("==================================================")
        os.makedirs("data", exist_ok=True)
        df = pd.DataFrame(collected_data)
        
        cols = ['date', 'jcd', 'rno', 'wind', 'res1', 'rank1', 'rank2', 'rank3', 'payout']
        for i in range(1, 7):
            cols.extend([f'wr{i}', f'mo{i}', f'ex{i}', f'f{i}', f'st{i}'])
        
        use_cols = [c for c in cols if c in df.columns]
        df = df[use_cols]

        output_path = "data/debug_result.csv"
        df.to_csv(output_path, index=False)
        log(f"ğŸ‰ å®Œäº†ï¼ {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    else:
        log("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒ1ä»¶ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        sys.exit(1)
