import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import sys
import argparse
import os
import re
import threading
import unicodedata
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

# ãƒ­ã‚°è¨­å®š
sys.stdout.reconfigure(line_buffering=True)
print_lock = threading.Lock()

def safe_print(msg):
    with print_lock:
        print(msg)

# ==========================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
MAX_RETRIES = 3       # ãƒªãƒˆãƒ©ã‚¤å›žæ•°
RETRY_INTERVAL = 2    # ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿæ™‚é–“(ç§’)
MAX_WORKERS = 8       # ä¸¦åˆ—æ•°ï¼ˆGitHub Actionsãªã‚‰8-10æŽ¨å¥¨ï¼‰

def get_session():
    """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    session = requests.Session()
    adapter = HTTPAdapter(
        pool_connections=MAX_WORKERS,
        pool_maxsize=MAX_WORKERS,
        max_retries=Retry(total=MAX_RETRIES, backoff_factor=1)
    )
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    })
    return session

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ï¼ˆå…¨è§’â†’åŠè§’ã€ã‚«ãƒ³ãƒžãƒ»å††ãƒžãƒ¼ã‚¯å‰Šé™¤ï¼‰"""
    if not text: return ""
    text = unicodedata.normalize('NFKC', str(text))
    return text.replace("\n", "").replace("\r", "").replace(" ", "").replace("Â¥", "").replace(",", "").strip()

def get_soup_diagnostic(session, url, check_selector=None):
    """HTMLå–å¾—ï¼†è¨ºæ–­ï¼ˆé–‹å‚¬ãªã—åˆ¤å®šä»˜ãï¼‰"""
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            res = session.get(url, timeout=15)
            res.encoding = res.apparent_encoding
            
            if res.status_code == 200:
                # é–‹å‚¬ãªã—åˆ¤å®š
                if "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in res.text:
                    return None, "SKIP"

                soup = BeautifulSoup(res.text, 'html.parser')
                
                # ç‰¹å®šã®è¦ç´ ï¼ˆãƒ¬ãƒ¼ã‚¹æƒ…å ±ãªã©ï¼‰ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if check_selector and not soup.select_one(check_selector):
                    time.sleep(RETRY_INTERVAL)
                    continue 
                return soup, None
        except:
            time.sleep(RETRY_INTERVAL)
            
    return None, "ERROR"

def scrape_race_data(session, jcd, rno, date_str):
    """1ãƒ¬ãƒ¼ã‚¹åˆ†ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    # 1. ç›´å‰æƒ…å ±ï¼ˆã¾ãšã“ã‚Œã§é–‹å‚¬æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
    soup_before, err = get_soup_diagnostic(
        session, 
        f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}",
        check_selector=".is-boatColor1"
    )
    if err == "SKIP" or not soup_before:
        return None

    # 2. çµæžœï¼ˆç€é †ã€é…å½“ï¼‰
    soup_res, err = get_soup_diagnostic(session, f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    if not soup_res: return None

    # 3. ç•ªçµ„è¡¨ï¼ˆFæ•°ã€STã€å‹çŽ‡ã€ãƒ¢ãƒ¼ã‚¿ãƒ¼ï¼‰
    soup_list, err = get_soup_diagnostic(session, f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    if not soup_list: return None

    try:
        row = {'date': date_str, 'jcd': jcd, 'rno': rno}

        # --- â‘  é¢¨é€Ÿ ---
        try:
            wind_elem = soup_before.select_one(".weather1_bodyUnitLabelData")
            if wind_elem:
                row['wind'] = float(clean_text(wind_elem.text).replace("m", ""))
            else:
                row['wind'] = 0.0
        except: row['wind'] = 0.0

        # --- â‘¡ ç€é † (rank1~3) ---
        # åˆæœŸå€¤
        row['rank1'], row['rank2'], row['rank3'] = None, None, None
        try:
            rank_rows = soup_res.select("table.is-w495 tbody tr")
            for r in rank_rows:
                tds = r.select("td")
                if len(tds) > 1:
                    rank_idx = clean_text(tds[0].text)
                    boat_num = clean_text(tds[1].text)
                    if rank_idx.isdigit() and int(rank_idx) <= 3:
                        row[f'rank{rank_idx}'] = int(boat_num)
        except: pass

        # æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›ç”¨ (1å·è‰‡ãŒ1ç€ãªã‚‰1)
        row['res1'] = 1 if row.get('rank1') == 1 else 0

        # --- â‘¢ 3é€£å˜é…å½“ (payout) ---
        row['payout'] = 0
        try:
            # "3é€£å˜"ã‚’å«ã‚€thã‚’æŽ¢ã™
            payout_th = soup_res.find(lambda tag: tag.name == "th" and "3é€£å˜" in tag.text)
            if payout_th:
                # éš£ã®éš£ã®ã‚»ãƒ«ãŒæ‰•æˆ»é‡‘
                payout_td = payout_th.find_next_sibling("td").find_next_sibling("td")
                if payout_td:
                    val = clean_text(payout_td.text)
                    if val.isdigit():
                        row['payout'] = int(val)
        except: pass

        # --- â‘£ å„è‰‡ãƒ‡ãƒ¼ã‚¿ (Fæ•°, ST, ãƒ¢ãƒ¼ã‚¿ãƒ¼ç­‰) ---
        for i in range(1, 7):
            # [ç›´å‰æƒ…å ±] å±•ç¤ºã‚¿ã‚¤ãƒ 
            try:
                boat_cell = soup_before.select_one(f".is-boatColor{i}")
                if boat_cell:
                    tds = boat_cell.find_parent("tbody").select("td")
                    ex_val = clean_text(tds[4].text)
                    row[f'ex{i}'] = float(ex_val) if ex_val and ex_val != "." else 0.0
                else: row[f'ex{i}'] = 0.0
            except: row[f'ex{i}'] = 0.0

            # [ç•ªçµ„è¡¨] è©³ç´°ãƒ‡ãƒ¼ã‚¿
            try:
                list_tbody = soup_list.select_one(f".is-boatColor{i}").find_parent("tbody")
                tds = list_tbody.select("td")
                
                # å…¨å›½å‹çŽ‡ (tds[3])
                wr_match = re.search(r"(\d\.\d{2})", clean_text(tds[3].text))
                row[f'wr{i}'] = float(wr_match.group(1)) if wr_match else 0.0
                
                # Fæ•° (é¸æ‰‹åæ¬„ tds[2] ã‹ã‚‰ "F1" ç­‰ã‚’æŠ½å‡º)
                f_match = re.search(r"F(\d+)", clean_text(tds[2].text))
                row[f'f{i}'] = int(f_match.group(1)) if f_match else 0
                
                # å¹³å‡ST (tds[3] ã¾ãŸã¯è¡Œå…¨ä½“ã‹ã‚‰ "ST0.15" ã‚’æŽ¢ã™)
                st_match = re.search(r"ST(\d\.\d{2})", list_tbody.text.replace("\n", ""))
                row[f'st{i}'] = float(st_match.group(1)) if st_match else 0.17
                
                # ãƒ¢ãƒ¼ã‚¿ãƒ¼2é€£çŽ‡ (tds[5] ã¾ãŸã¯ tds[6] ã‹ã‚‰ "%" ã®ã¤ã„ãŸæ•°å­—ã‚’æŠ½å‡º)
                mo_text = clean_text(tds[5].text) # é€šå¸¸ã¯ã“ã“
                mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                if not mo_match:
                    mo_text = clean_text(tds[6].text) # å¿µã®ãŸã‚éš£ã‚‚ãƒã‚§ãƒƒã‚¯
                    mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                row[f'mo{i}'] = float(mo_match.group(1)) if mo_match else 0.0
                
            except:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®å®‰å…¨å€¤
                row[f'wr{i}'], row[f'f{i}'], row[f'st{i}'], row[f'mo{i}'] = 0.0, 0, 0.20, 0.0

        return row

    except Exception as e:
        return None

def process_race_parallel(args):
    """ä¸¦åˆ—å‡¦ç†ç”¨ãƒ©ãƒƒãƒ‘ãƒ¼"""
    time.sleep(0.1) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å¾®å°ã‚¦ã‚§ã‚¤ãƒˆ
    return scrape_race_data(*args)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", required=True)
    parser.add_argument("--end", required=True)
    args = parser.parse_args()

    os.makedirs("data", exist_ok=True)
    session = get_session()
    
    start_d = datetime.strptime(args.start, "%Y-%m-%d")
    end_d = datetime.strptime(args.end, "%Y-%m-%d")
    current = start_d
    
    print(f"ðŸš€ åŽé›†é–‹å§‹: {args.start} ã€œ {args.end}")
    
    # é€æ¬¡ä¿å­˜ç”¨ãƒ•ã‚¡ã‚¤ãƒ«å
    filename = f"data/chunk_{args.start.replace('-','')}.csv"
    file_exists = False
    
    while current <= end_d:
        d_str = current.strftime("%Y%m%d")
        print(f"ðŸ“… {d_str} å‡¦ç†ä¸­...")
        
        # 1æ—¥åˆ†ã®å…¨ãƒ¬ãƒ¼ã‚¹ã‚¿ã‚¹ã‚¯ä½œæˆ
        tasks = [(session, jcd, rno, d_str) for jcd in range(1, 25) for rno in range(1, 13)]
        
        day_results = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            for res in executor.map(process_race_parallel, tasks):
                if res: day_results.append(res)
        
        if day_results:
            df = pd.DataFrame(day_results)
            
            # ã‚«ãƒ©ãƒ é †åºã‚’æ•´ç†ï¼ˆè¦‹ã‚„ã™ãã™ã‚‹ï¼‰
            cols = ['date', 'jcd', 'rno', 'wind', 'res1', 'rank1', 'rank2', 'rank3', 'payout']
            for i in range(1, 7):
                cols.extend([f'wr{i}', f'mo{i}', f'ex{i}', f'f{i}', f'st{i}'])
            
            # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã ã‘æŠ½å‡º
            use_cols = [c for c in cols if c in df.columns]
            df = df[use_cols]

            # è¿½è¨˜ä¿å­˜
            df.to_csv(filename, mode='a', index=False, header=not file_exists)
            file_exists = True
            safe_print(f"  âœ… {len(day_results)}ãƒ¬ãƒ¼ã‚¹ ä¿å­˜å®Œäº†")
        else:
            safe_print("  âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—")
        
        current += timedelta(days=1)

    print("ðŸŽ‰ å…¨æœŸé–“å®Œäº†")
