import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import sys
import argparse
import os
import traceback
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# ğŸ”¥ ãƒ­ã‚°ã‚’å¼·åˆ¶çš„ã«å³æ™‚è¡¨ç¤ºã•ã›ã‚‹ãŠã¾ã˜ãªã„
sys.stdout.reconfigure(line_buffering=True)

# ==========================================
# âš™ï¸ è¨­å®šï¼ˆé«˜é€Ÿä¸¦åˆ—ãƒ¢ãƒ¼ãƒ‰ï¼‰
# ==========================================
MAX_RETRIES = 3
RETRY_INTERVAL = 3
MAX_WORKERS = 8  # 8ä¼šå ´åŒæ™‚ã«æ”»ã‚ã‚‹ï¼

def get_session():
    """äººé–“ã‚‰ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
    session = requests.Session()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Referer': 'https://www.boatrace.jp/',
    }
    session.headers.update(headers)
    return session

def get_soup_with_retry(session, url):
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            res = session.get(url, timeout=20) # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å°‘ã—çŸ­ãã—ã¦å›è»¢ç‡ã‚’ä¸Šã’ã‚‹
            res.encoding = res.apparent_encoding
            if res.status_code == 200:
                return BeautifulSoup(res.text, 'html.parser')
            elif res.status_code == 403:
                print(f"â›” 403 Forbidden: {url}")
        except:
            pass
        if attempt < MAX_RETRIES:
            time.sleep(RETRY_INTERVAL)
    return None

def scrape_race_data(session, jcd, rno, date_str):
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    # 3ãƒšãƒ¼ã‚¸å–å¾—
    soup_list = get_soup_with_retry(session, f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    soup_before = get_soup_with_retry(session, f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    soup_res = get_soup_with_retry(session, f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}")

    if not all([soup_list, soup_before, soup_res]):
        return None

    try:
        # é¢¨é€Ÿå–å¾—ï¼ˆä¿®æ­£æ¸ˆã¿ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        wind = 0.0
        weather_units = soup_before.select('.weather1_bodyUnit')
        for unit in weather_units:
            title_elem = unit.select_one('.weather1_bodyUnitLabelTitle')
            if title_elem and 'é¢¨é€Ÿ' in title_elem.text:
                data_elem = unit.select_one('.weather1_bodyUnitLabelData')
                if data_elem:
                    try:
                        wind = float(data_elem.text.strip().replace('m', ''))
                    except:
                        pass
                break

        # æ­£è§£ãƒ©ãƒ™ãƒ«
        res1_text = soup_res.select_one('.is-p_0-1 .is-p_1-1') 
        res1 = 1 if (res1_text and res1_text.text.strip() == "1") else 0

        # å±•ç¤ºã‚¿ã‚¤ãƒ  & æ¬ æãƒã‚§ãƒƒã‚¯
        temp_ex_times = []
        for i in range(1, 7):
            ex_elem = soup_before.select(f'tbody.is-p_0-{i}')
            if not ex_elem: return None
            ex_val = ex_elem[0].select('td')[4].text.strip()
            if not ex_val or ex_val == "-" or float(ex_val) <= 0:
                return None
            temp_ex_times.append(float(ex_val))

        # ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        row = {'date': date_str, 'jcd': jcd, 'rno': rno, 'wind': wind, 'res1': res1}
        for i in range(1, 7):
            tbody = soup_list.select(f'tbody.is-p_0-{i}')[0].select('td')
            row[f'wr{i}'] = float(tbody[3].select_one('div').text.split()[0])
            row[f'mo{i}'] = float(tbody[6].select_one('div').text.split()[0])
            row[f'ex{i}'] = temp_ex_times[i-1]
        return row

    except:
        return None

def process_stadium(args):
    """ä¼šå ´å˜ä½ã§å‡¦ç†ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°"""
    session, jcd, date_str = args
    results = []
    # ãƒ­ã‚°ã‚’å‡ºã—ã¦ç”Ÿå­˜ç¢ºèª
    print(f"ğŸŸï¸ ä¼šå ´{jcd:02d} ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹...", flush=True)
    
    for rno in range(1, 13):
        data = scrape_race_data(session, jcd, rno, date_str)
        if data:
            results.append(data)
            
    print(f"âœ… ä¼šå ´{jcd:02d} å®Œäº† ({len(results)}ãƒ¬ãƒ¼ã‚¹)", flush=True)
    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", required=True)
    parser.add_argument("--end", required=True)
    args = parser.parse_args()

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³æº–å‚™
    print("ğŸš€ é«˜é€Ÿåé›†ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆèµ·å‹•", flush=True)
    session = get_session()
    
    # æœ€åˆã«ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’è¸ã‚“ã§Cookieã‚²ãƒƒãƒˆ
    try:
        get_soup_with_retry(session, "https://www.boatrace.jp/")
        print("ğŸ”“ èªè¨¼çªç ´æˆåŠŸã€‚ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ã‚’é–‹å§‹ã—ã¾ã™...", flush=True)
    except:
        print("âš ï¸ ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—ï¼ˆç¶šè¡Œã—ã¾ã™ï¼‰")

    start_d = datetime.strptime(args.start, "%Y-%m-%d")
    end_d = datetime.strptime(args.end, "%Y-%m-%d")
    current = start_d
    
    while current <= end_d:
        d_str = current.strftime("%Y%m%d")
        print(f"ğŸ“… æ—¥ä»˜å‡¦ç†ä¸­: {d_str}", flush=True)
        
        # ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¼šå ´ã”ã¨ã«ä¸¦åˆ—å®Ÿè¡Œ
        # å¼•æ•°ãƒªã‚¹ãƒˆä½œæˆ: (session, ä¼šå ´ã‚³ãƒ¼ãƒ‰, æ—¥ä»˜)
        tasks = [(session, jcd, d_str) for jcd in range(1, 25)]
        
        day_results = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # å®Ÿè¡Œçµæœã‚’å—ã‘å–ã‚‹
            futures = executor.map(process_stadium, tasks)
            for res in futures:
                day_results.extend(res)
        
        if day_results:
            df = pd.DataFrame(day_results)
            os.makedirs("data", exist_ok=True)
            filename = f"data/pure_data_{d_str}.csv"
            df.to_csv(filename, index=False)
            print(f"ğŸ’¾ {d_str} ä¿å­˜å®Œäº†: {len(df)}ãƒ¬ãƒ¼ã‚¹", flush=True)
        else:
            print(f"âš ï¸ {d_str} ãƒ‡ãƒ¼ã‚¿ãªã—", flush=True)
            
        current += timedelta(days=1)
