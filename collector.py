import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import sys
import argparse
import os
import traceback
from datetime import datetime, timedelta

# ãƒ­ã‚°ã‚’å³æ™‚è¡¨ç¤ºï¼ˆã“ã‚ŒãŒãªã„ã¨Actionsã§ãƒ­ã‚°ãŒé…ã‚Œã‚‹ï¼‰
sys.stdout.reconfigure(line_buffering=True)

# ==========================================
# âš™ï¸ è¨­å®šï¼ˆ1ä¼šå ´ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆè¨ºæ–­ï¼‰
# ==========================================
MAX_RETRIES = 3
RETRY_INTERVAL = 3
TARGET_JCD = 1  # 01:æ¡ç”Ÿ ã ã‘ã‚’ãƒ†ã‚¹ãƒˆ

def get_session():
    session = requests.Session()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Referer': 'https://www.boatrace.jp/',
    }
    session.headers.update(headers)
    return session

def get_soup_with_retry(session, url):
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            res = session.get(url, timeout=10)
            res.encoding = res.apparent_encoding
            if res.status_code == 200:
                return BeautifulSoup(res.text, 'html.parser')
            elif res.status_code == 403:
                print(f"â›” 403 Forbidden (ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™): {url}")
        except Exception as e:
            print(f"âš ï¸ é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
        time.sleep(RETRY_INTERVAL)
    return None

def scrape_race_data(session, jcd, rno, date_str):
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    # 3ãƒšãƒ¼ã‚¸å–å¾—
    print(f"ğŸ” {jcd}å ´ {rno}R: ã‚¢ã‚¯ã‚»ã‚¹é–‹å§‹...", end=" ")
    soup_list = get_soup_with_retry(session, f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    soup_before = get_soup_with_retry(session, f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    soup_res = get_soup_with_retry(session, f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}")

    if not all([soup_list, soup_before, soup_res]):
        print("âŒ ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•— (HTMLãªã—)")
        return None

    try:
        # é¢¨é€Ÿãƒã‚§ãƒƒã‚¯
        wind = 0.0
        try:
            weather_units = soup_before.select('.weather1_bodyUnit')
            for unit in weather_units:
                title_elem = unit.select_one('.weather1_bodyUnitLabelTitle')
                if title_elem and 'é¢¨é€Ÿ' in title_elem.text:
                    data_elem = unit.select_one('.weather1_bodyUnitLabelData')
                    if data_elem:
                        wind = float(data_elem.text.strip().replace('m', ''))
                    break
        except:
            print("âš ï¸ é¢¨é€Ÿã‚¨ãƒ©ãƒ¼(ç„¡è¦–)", end=" ")

        # æ­£è§£ãƒ©ãƒ™ãƒ«
        res1 = 0
        try:
            res1_text = soup_res.select_one('.is-p_0-1 .is-p_1-1') 
            res1 = 1 if (res1_text and res1_text.text.strip() == "1") else 0
        except:
            pass

        # å±•ç¤ºã‚¿ã‚¤ãƒ ãƒã‚§ãƒƒã‚¯ï¼ˆã“ã“ãŒä¸€ç•ªã‚ã‚„ã—ã„ï¼‰
        temp_ex_times = []
        for i in range(1, 7):
            ex_elem = soup_before.select(f'tbody.is-p_0-{i}')
            if not ex_elem:
                print(f"âŒ {i}å·è‰‡ãƒ‡ãƒ¼ã‚¿ãªã—", end=" ")
                return None
            
            ex_val = ex_elem[0].select('td')[4].text.strip()
            
            # è©³ç´°ãƒ­ã‚°å‡ºåŠ›
            if not ex_val or ex_val == "-" or ex_val == "0.00":
                print(f"âŒ {i}å·è‰‡å±•ç¤ºæ¬ æ[{ex_val}]", end=" ")
                return None
            
            try:
                val = float(ex_val)
                if val <= 0:
                    print(f"âŒ {i}å·è‰‡å±•ç¤ºç•°å¸¸[{val}]", end=" ")
                    return None
                temp_ex_times.append(val)
            except:
                print(f"âŒ {i}å·è‰‡æ•°å€¤å¤‰æ›ä¸å¯[{ex_val}]", end=" ")
                return None

        # ã“ã“ã¾ã§æ¥ã‚Œã°æˆåŠŸ
        # ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        row = {'date': date_str, 'jcd': jcd, 'rno': rno, 'wind': wind, 'res1': res1}
        for i in range(1, 7):
            tbody = soup_list.select(f'tbody.is-p_0-{i}')[0].select('td')
            row[f'wr{i}'] = float(tbody[3].select_one('div').text.split()[0])
            row[f'mo{i}'] = float(tbody[6].select_one('div').text.split()[0])
            row[f'ex{i}'] = temp_ex_times[i-1]
            
        print("âœ… æˆåŠŸï¼")
        return row

    except Exception as e:
        print(f"ğŸ’¥ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", required=True)
    parser.add_argument("--end", required=True)
    args = parser.parse_args()

    os.makedirs("data", exist_ok=True)
    session = get_session()
    
    # èªè¨¼ç”¨ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
    get_soup_with_retry(session, "https://www.boatrace.jp/")

    start_d = datetime.strptime(args.start, "%Y-%m-%d")
    end_d = datetime.strptime(args.end, "%Y-%m-%d")
    current = start_d
    results = []

    print(f"ğŸš€ 1ä¼šå ´é™å®šãƒ‡ãƒãƒƒã‚°: {args.start}")

    while current <= end_d:
        d_str = current.strftime("%Y%m%d")
        
        # ğŸ”¥ TARGET_JCDï¼ˆ01:æ¡ç”Ÿï¼‰ã ã‘ã‚’å›ã™
        jcd = TARGET_JCD 
        print(f"ğŸŸï¸ {d_str} ä¼šå ´{jcd:02d} ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¾ã™")
        
        for rno in range(1, 13):
            data = scrape_race_data(session, jcd, rno, d_str)
            if data:
                results.append(data)
            time.sleep(1) # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            
        current += timedelta(days=1)

    if results:
        df = pd.DataFrame(results)
        filename = f"data/pure_data_debug.csv"
        df.to_csv(filename, index=False)
        print(f"\nâœ¨ å®Œäº†: {len(df)}ãƒ¬ãƒ¼ã‚¹å–å¾—ã—ã¾ã—ãŸã€‚")
    else:
        print("\nğŸ’€ å…¨ãƒ¬ãƒ¼ã‚¹å¤±æ•—ã—ã¾ã—ãŸã€‚ä¸Šã®ãƒ­ã‚°ã® âŒ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
