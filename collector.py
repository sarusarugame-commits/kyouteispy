import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import sys
import argparse
import os
import re
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

# ãƒ­ã‚°ã‚’å³æ™‚è¡¨ç¤º
sys.stdout.reconfigure(line_buffering=True)

# ==========================================
# âš™ï¸ è¨­å®šï¼ˆçˆ†é€Ÿä¸¦åˆ—ãƒ¢ãƒ¼ãƒ‰ï¼‰
# ==========================================
MAX_RETRIES = 3
RETRY_INTERVAL = 3
MAX_WORKERS = 8 # 1ã¤ã®ä¼šå ´ã§åŒæ™‚ã«å‡¦ç†ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°

def get_session():
    session = requests.Session()
    # ä¸¦åˆ—å‡¦ç†ç”¨ã«æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’å¢—ã‚„ã™
    adapter = HTTPAdapter(
        pool_connections=MAX_WORKERS,
        pool_maxsize=MAX_WORKERS,
        max_retries=Retry(total=MAX_RETRIES, backoff_factor=1)
    )
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    }
    session.headers.update(headers)
    return session

def get_soup_with_retry(session, url):
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            res = session.get(url, timeout=20) # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯å°‘ã—çŸ­ã‚ã«
            res.encoding = res.apparent_encoding
            if res.status_code == 200:
                return BeautifulSoup(res.text, 'html.parser')
        except:
            pass
        time.sleep(RETRY_INTERVAL)
    return None

def clean_text(text):
    if not text: return ""
    return text.replace("\n", "").replace("\r", "").replace(" ", "").replace("\u3000", "").strip()

def scrape_race_data(session, jcd, rno, date_str):
    """ä¿®æ­£æ¸ˆã¿ã®æœ€å¼·ãƒ‘ãƒ¼ã‚¹ãƒ­ã‚¸ãƒƒã‚¯"""
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    # 3ãƒšãƒ¼ã‚¸å–å¾—
    soup_list = get_soup_with_retry(session, f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    soup_before = get_soup_with_retry(session, f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    soup_res = get_soup_with_retry(session, f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}")

    if not all([soup_list, soup_before, soup_res]):
        return None

    try:
        # --- 1. é¢¨é€Ÿå–å¾— ---
        wind = 0.0
        try:
            wind_elem = soup_before.find(string=re.compile("é¢¨é€Ÿ"))
            if wind_elem:
                parent = wind_elem.find_parent(class_="weather1_bodyUnit")
                if parent:
                    data_elem = parent.select_one(".weather1_bodyUnitLabelData")
                    if data_elem:
                        w_text = clean_text(data_elem.text).replace("m", "")
                        wind = float(w_text)
        except:
            pass 

        # --- 2. æ­£è§£ãƒ©ãƒ™ãƒ« (1ç€) ---
        res1 = 0
        try:
            res_rows = soup_res.select(".is-p_1-1")
            if res_rows:
                rank1_boat = clean_text(res_rows[0].select("td")[1].text)
                if rank1_boat == "1":
                    res1 = 1
        except:
            pass

        # --- 3. å±•ç¤ºã‚¿ã‚¤ãƒ  & å„è‰‡ãƒ‡ãƒ¼ã‚¿ ---
        temp_ex_times = []
        for i in range(1, 7):
            # è‰‡ç•ªã®è‰²ã‚¯ãƒ©ã‚¹ã‹ã‚‰æ¢ã™ç¢ºå®Ÿãªæ–¹æ³•
            boat_cell = soup_before.select_one(f".is-boatColor{i}")
            if not boat_cell: return None
            
            tbody = boat_cell.find_parent("tbody")
            tds = tbody.select("td")
            
            ex_val = clean_text(tds[4].text)
            if not ex_val: ex_val = clean_text(tds[5].text)
            
            if not ex_val or ex_val == "-" or ex_val == "0.00":
                return None
            temp_ex_times.append(float(ex_val))

        # --- 4. å‡ºèµ°è¡¨ãƒ‡ãƒ¼ã‚¿ ---
        row = {'date': date_str, 'jcd': jcd, 'rno': rno, 'wind': wind, 'res1': res1}
        for i in range(1, 7):
            boat_cell_list = soup_list.select_one(f".is-boatColor{i}")
            if not boat_cell_list: return None
            
            tbody_list = boat_cell_list.find_parent("tbody")
            tds_list = tbody_list.select("td")
            
            try:
                row[f'wr{i}'] = float(re.findall(r"\d+\.\d+", tds_list[3].text)[0])
                nums = re.findall(r"\d+\.\d+", tds_list[6].text)
                row[f'mo{i}'] = float(nums[0]) if len(nums) >= 1 else 0.0
            except:
                row[f'wr{i}'] = 0.0
                row[f'mo{i}'] = 0.0

            row[f'ex{i}'] = temp_ex_times[i-1]

        return row

    except:
        return None

def process_race_parallel(args):
    """ä¸¦åˆ—å‡¦ç†ç”¨ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°"""
    session, jcd, rno, date_str = args
    return scrape_race_data(session, jcd, rno, date_str)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", required=True)
    parser.add_argument("--end", required=True)
    args = parser.parse_args()

    os.makedirs("data", exist_ok=True)
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆãƒ—ãƒ¼ãƒ«æ•°å¢—å¼·æ¸ˆã¿ï¼‰
    session = get_session()
    try:
        session.get("https://www.boatrace.jp/", timeout=10)
    except:
        pass

    start_d = datetime.strptime(args.start, "%Y-%m-%d")
    end_d = datetime.strptime(args.end, "%Y-%m-%d")
    current = start_d
    
    print(f"ğŸš€ ä¸¦åˆ—ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹: {args.start} ã€œ {args.end}")
    
    results = []
    
    while current <= end_d:
        d_str = current.strftime("%Y%m%d")
        print(f"ğŸ“… {d_str} ä¸¦åˆ—ã‚¹ã‚­ãƒ£ãƒ³ä¸­...", end="", flush=True)
        
        # 1æ—¥åˆ†ã®å…¨ãƒ¬ãƒ¼ã‚¹ï¼ˆ24ä¼šå ´Ã—12ãƒ¬ãƒ¼ã‚¹ï¼‰ã®ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
        tasks = []
        for jcd in range(1, 25):
            for rno in range(1, 13):
                tasks.append((session, jcd, rno, d_str))
        
        # ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œï¼
        day_results = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = executor.map(process_race_parallel, tasks)
            for res in futures:
                if res:
                    day_results.append(res)
        
        print(f" -> {len(day_results)}ãƒ¬ãƒ¼ã‚¹å–å¾—")
        results.extend(day_results)
        
        current += timedelta(days=1)

    if results:
        df = pd.DataFrame(results)
        filename = f"data/pure_data_{args.start}_{args.end}.csv"
        df.to_csv(filename, index=False)
        print(f"ğŸ‰ å…¨å·¥ç¨‹å®Œäº†ï¼CSVä¿å­˜: {filename} ({len(df)}è¡Œ)")
    else:
        print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
