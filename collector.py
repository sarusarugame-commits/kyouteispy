import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import sys
import argparse
import os
import re
import threading
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from requests.adapters import HTTPAdapter
from urllib3.util import Retry

# ãƒ­ã‚°ã‚’å³æ™‚è¡¨ç¤ºï¼ˆGitHub Actionsç”¨ï¼‰
sys.stdout.reconfigure(line_buffering=True)
print_lock = threading.Lock()

def safe_print(msg):
    with print_lock:
        print(msg)

# ==========================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
MAX_RETRIES = 5       # ãƒªãƒˆãƒ©ã‚¤å›æ•°
RETRY_INTERVAL = 5    # é€šå¸¸ãƒªãƒˆãƒ©ã‚¤æ™‚ã®å¾…æ©Ÿæ™‚é–“
BAN_WAIT_TIME = 20    # â›” BAN/ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™æ¤œçŸ¥æ™‚ã®å¾…æ©Ÿæ™‚é–“
MAX_WORKERS = 2       # å®‰å…¨ã®ãŸã‚ã€Œ2ã€æ¨å¥¨ï¼ˆå¢—ã‚„ã—ã™ãã‚‹ã¨è¨ºæ–­ãƒ­ã‚°ã§ã‚¨ãƒ©ãƒ¼ãŒåŸ‹ã‚å°½ãã•ã‚Œã¾ã™ï¼‰

def get_session():
    session = requests.Session()
    adapter = HTTPAdapter(
        pool_connections=MAX_WORKERS,
        pool_maxsize=MAX_WORKERS,
        max_retries=Retry(total=MAX_RETRIES, backoff_factor=1)
    )
    session.mount("https://", adapter)
    session.mount("http://", adapter)
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    })
    return session

def clean_text(text):
    if not text: return ""
    return text.replace("\n", "").replace("\r", "").replace(" ", "").replace("\u3000", "").strip()

def get_soup_diagnostic(session, url, check_selector=None):
    """
    HTMLã‚’å–å¾—ã—ã€å†…å®¹ã‚’è¨ºæ–­ã—ã¦è¿”ã™é–¢æ•°
    Returns: (soup, error_message)
    - æˆåŠŸæ™‚: (soup_object, None)
    - å¤±æ•—æ™‚: (None, "ã‚¨ãƒ©ãƒ¼è©³ç´°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    """
    last_error = ""
    
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            res = session.get(url, timeout=30)
            res.encoding = res.apparent_encoding
            
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                
                # ãƒã‚§ãƒƒã‚¯è¦ç´ ï¼ˆä¾‹ï¼š.is-boatColor1ï¼‰ãŒã‚ã‚‹ã‹ç¢ºèª
                if check_selector:
                    if not soup.select_one(check_selector):
                        # 200 OK ã ãŒä¸­èº«ãŒé•ã†ï¼ˆã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ç­‰ï¼‰
                        page_title = clean_text(soup.title.text) if soup.title else "No Title"
                        body_sample = clean_text(soup.body.text)[:50] if soup.body else "No Body"
                        
                        err_msg = f"â›” è§£æå¤±æ•—ï¼ˆä¸­èº«ãŒä¸æ­£ï¼‰ Title:ã€{page_title}ã€‘ Text: {body_sample}..."
                        
                        # ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ç³»ãªã‚‰å¾…æ©Ÿ
                        if "ã‚¢ã‚¯ã‚»ã‚¹" in page_title or "Error" in page_title:
                            safe_print(f"   ğŸ›¡ï¸ ãƒ–ãƒ­ãƒƒã‚¯æ¤œçŸ¥ã€‚{BAN_WAIT_TIME}ç§’å¾…æ©Ÿã—ã¾ã™...")
                            time.sleep(BAN_WAIT_TIME * attempt)
                        
                        last_error = err_msg
                        continue # ãƒªãƒˆãƒ©ã‚¤ã¸
                
                # æ­£å¸¸
                return soup, None
            
            else:
                last_error = f"HttpError: {res.status_code}"
                
        except Exception as e:
            last_error = f"ConnectionError: {e}"
            
        time.sleep(RETRY_INTERVAL)
    
    return None, last_error

def scrape_race_data(session, jcd, rno, date_str):
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    log_prefix = f"{date_str} J{jcd:02} R{rno:02}"
    
    # 1. ç›´å‰æƒ…å ±ï¼ˆã“ã“ã«ä¸€ç•ªé‡è¦ãªãƒ‡ãƒ¼ã‚¿ãŒå¤šã„ã®ã§æœ€åˆã«ãƒã‚§ãƒƒã‚¯ï¼‰
    soup_before, err = get_soup_diagnostic(
        session, 
        f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}",
        check_selector=".is-boatColor1" # ã“ã‚ŒãŒãªã„ã¨è©±ã«ãªã‚‰ãªã„
    )
    
    if not soup_before:
        # å¤±æ•—ãƒ­ã‚°ï¼ˆã“ã“ã§ã€Œãªãœãƒ€ãƒ¡ã ã£ãŸã‹ã€ãŒå‡ºã‚‹ï¼‰
        safe_print(f"âŒ {log_prefix}: ç›´å‰æƒ…å ±å–å¾—å¤±æ•— -> {err}")
        return None

    # 2. ç•ªçµ„è¡¨
    soup_list, err = get_soup_diagnostic(session, f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    if not soup_list:
        safe_print(f"âŒ {log_prefix}: ç•ªçµ„è¡¨å–å¾—å¤±æ•— -> {err}")
        return None

    # 3. çµæœ
    soup_res, err = get_soup_diagnostic(session, f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}")
    if not soup_res:
        safe_print(f"âŒ {log_prefix}: çµæœå–å¾—å¤±æ•— -> {err}")
        return None

    try:
        # --- ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ ---
        
        # é¢¨é€Ÿ
        wind = 0.0
        try:
            wind_elem = soup_before.find(string=re.compile("é¢¨é€Ÿ"))
            if wind_elem:
                parent = wind_elem.find_parent(class_="weather1_bodyUnit")
                if parent:
                    data_elem = parent.select_one(".weather1_bodyUnitLabelData")
                    if data_elem:
                        wind = float(clean_text(data_elem.text).replace("m", ""))
        except: pass 

        # 1ç€ãƒ•ãƒ©ã‚°
        res1 = 0
        try:
            res_rows = soup_res.select(".is-p_1-1")
            if res_rows:
                rank1_boat = clean_text(res_rows[0].select("td")[1].text)
                if rank1_boat == "1":
                    res1 = 1
        except: pass

        # å±•ç¤ºã‚¿ã‚¤ãƒ 
        temp_ex_times = []
        for i in range(1, 7):
            boat_cell = soup_before.select_one(f".is-boatColor{i}")
            if not boat_cell:
                # äº‹å‰ãƒã‚§ãƒƒã‚¯ã‚’é€šã£ã¦ã„ã‚‹ã®ã§ã“ã“ã¯èµ·ãã«ãã„ã¯ãš
                safe_print(f"âš ï¸ {log_prefix}: æ§‹é€ ã‚¨ãƒ©ãƒ¼ï¼ˆ{i}å·è‰‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼‰")
                return None

            tbody = boat_cell.find_parent("tbody")
            tds = tbody.select("td")
            
            # å±•ç¤ºã‚¿ã‚¤ãƒ å–å¾—ï¼ˆåˆ—ã‚ºãƒ¬å¯¾å¿œï¼‰
            ex_val = clean_text(tds[4].text)
            if not ex_val: ex_val = clean_text(tds[5].text)

            val_float = 0.0
            if ex_val and ex_val not in ["-", "0.00", "\xa0"]:
                try:
                    val_float = float(ex_val)
                except: pass
            temp_ex_times.append(val_float)

        # ãƒ‡ãƒ¼ã‚¿æ ¼ç´
        row = {'date': date_str, 'jcd': jcd, 'rno': rno, 'wind': wind, 'res1': res1}
        
        for i in range(1, 7):
            try:
                boat_cell_list = soup_list.select_one(f".is-boatColor{i}")
                if boat_cell_list:
                    tbody_list = boat_cell_list.find_parent("tbody")
                    tds_list = tbody_list.select("td")
                    
                    row[f'wr{i}'] = float(re.findall(r"\d+\.\d+", tds_list[3].text)[0])
                    nums = re.findall(r"\d+\.\d+", tds_list[6].text)
                    row[f'mo{i}'] = float(nums[0]) if nums else 0.0
                else:
                    row[f'wr{i}'], row[f'mo{i}'] = 0.0, 0.0
            except:
                row[f'wr{i}'], row[f'mo{i}'] = 0.0, 0.0

            row[f'ex{i}'] = temp_ex_times[i-1]

        safe_print(f"âœ… {log_prefix}: å®Œäº†")
        return row

    except Exception as e:
        safe_print(f"ğŸ’¥ {log_prefix}: ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ {e}")
        return None

def process_race_parallel(args):
    # BANå¯¾ç­–ã®ã‚¹ãƒªãƒ¼ãƒ—
    time.sleep(1.0)
    return scrape_race_data(*args)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", required=True)
    parser.add_argument("--end", required=True)
    args = parser.parse_args()

    os.makedirs("data", exist_ok=True)
    session = get_session()

    start_d = datetime.strptime(args.start, "%Y-%m-%d")
    end_d = datetime.strptime(args.end, "%Y-%m-%d")
    current = start_d

    print(f"ğŸš€ æœ¬ç•ªãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆè¨ºæ–­ãƒ­ã‚°ä»˜ï¼‰é–‹å§‹: {args.start} ã€œ {args.end}")
    
    results = []
    
    while current <= end_d:
        d_str = current.strftime("%Y%m%d")
        print(f"\n--- ğŸ“… {d_str} å‡¦ç†ä¸­ ---")
        
        tasks = []
        for jcd in range(1, 25):
            for rno in range(1, 13):
                tasks.append((session, jcd, rno, d_str))
        
        day_results = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = executor.map(process_race_parallel, tasks)
            for res in futures:
                if res:
                    day_results.append(res)
        
        print(f"ğŸ“Š {d_str}: {len(day_results)}ãƒ¬ãƒ¼ã‚¹å–å¾—")
        results.extend(day_results)
        current += timedelta(days=1)

    if results:
        df = pd.DataFrame(results)
        filename = f"data/chunk_{args.start}.csv"
        df.to_csv(filename, index=False)
        print(f"\nğŸ‰ å…¨å·¥ç¨‹å®Œäº†ï¼CSVä¿å­˜: {filename} ({len(df)}è¡Œ)")
    else:
        print("\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
