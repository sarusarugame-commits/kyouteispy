import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import re
import os
import unicodedata
import sys
import argparse

# ==========================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
# ãƒ‡ãƒãƒƒã‚°ç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ—¥ä»˜ï¼ˆå¼•æ•°ãŒãªã„å ´åˆã«ä½¿ç”¨ï¼‰
DEFAULT_TARGET_DATE = "20250101" 
# ä½•ãƒ¬ãƒ¼ã‚¹å–ã£ãŸã‚‰çµ‚äº†ã™ã‚‹ã‹
MAX_RACES = 5 

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ï¼ˆå…¨è§’â†’åŠè§’ã€ã‚«ãƒ³ãƒãƒ»å††ãƒãƒ¼ã‚¯å‰Šé™¤ï¼‰"""
    if not text: return ""
    text = unicodedata.normalize('NFKC', str(text))
    return text.replace("\n", "").replace("\r", "").replace(" ", "").replace("Â¥", "").replace(",", "").strip()

def get_soup(url):
    """HTMLå–å¾—ï¼ˆç°¡æ˜“ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    for _ in range(3): # 3å›ãƒªãƒˆãƒ©ã‚¤
        try:
            res = requests.get(url, headers=headers, timeout=10)
            res.encoding = res.apparent_encoding
            if res.status_code == 200:
                if "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“" in res.text: return None
                return BeautifulSoup(res.text, 'html.parser')
            time.sleep(1)
        except:
            time.sleep(1)
    return None

def scrape_race(jcd, rno, date_str):
    """1ãƒ¬ãƒ¼ã‚¹åˆ†ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    print(f"[{jcd}å ´ {rno}R] å–å¾—ä¸­...", end="")
    base_url = "https://www.boatrace.jp/owpc/pc/race"
    
    # URLç”Ÿæˆ
    url_before = f"{base_url}/beforeinfo?rno={rno}&jcd={jcd:02d}&hd={date_str}"
    url_res = f"{base_url}/raceresult?rno={rno}&jcd={jcd:02d}&hd={date_str}"
    url_list = f"{base_url}/racelist?rno={rno}&jcd={jcd:02d}&hd={date_str}"
    
    # ãƒšãƒ¼ã‚¸å–å¾—
    soup_before = get_soup(url_before)
    soup_res = get_soup(url_res)
    soup_list = get_soup(url_list)
    
    if not (soup_before and soup_res and soup_list):
        print(" ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒšãƒ¼ã‚¸å–å¾—ä¸å¯ï¼‰")
        return None

    try:
        row = {'date': date_str, 'jcd': jcd, 'rno': rno}

        # --- â‘  é¢¨é€Ÿ ---
        try:
            wind_elem = soup_before.select_one(".weather1_bodyUnitLabelData")
            row['wind'] = float(clean_text(wind_elem.text).replace("m", "")) if wind_elem else 0.0
        except: row['wind'] = 0.0

        # --- â‘¡ ç€é † (rank1, rank2, rank3) ---
        row['rank1'], row['rank2'], row['rank3'] = None, None, None
        try:
            rank_rows = soup_res.select("table.is-w495 tbody tr")
            for r in rank_rows:
                tds = r.select("td")
                if len(tds) > 1:
                    rank_idx = clean_text(tds[0].text)
                    boat_num = clean_text(tds[1].text)
                    if rank_idx.isdigit() and int(rank_idx) <= 3:
                        row[f'rank{rank_idx}'] = int(boat_num)
        except: pass
        
        # æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›ç”¨ (1å·è‰‡ãŒ1ç€ãªã‚‰1)
        row['res1'] = 1 if row.get('rank1') == 1 else 0

        # --- â‘¢ 3é€£å˜é…å½“ (payout) ---
        row['payout'] = 0
        try:
            payout_th = soup_res.find(lambda tag: tag.name == "th" and "3é€£å˜" in tag.text)
            if payout_th:
                payout_td = payout_th.find_next_sibling("td").find_next_sibling("td")
                if payout_td:
                    val = clean_text(payout_td.text)
                    if val.isdigit():
                        row['payout'] = int(val)
        except: pass

        # --- â‘£ å„è‰‡ãƒ‡ãƒ¼ã‚¿ (Fæ•°, ST, ãƒ¢ãƒ¼ã‚¿ãƒ¼ç­‰) ---
        for i in range(1, 7):
            # [ç›´å‰æƒ…å ±] å±•ç¤ºã‚¿ã‚¤ãƒ 
            try:
                boat_cell = soup_before.select_one(f".is-boatColor{i}")
                if boat_cell:
                    tds = boat_cell.find_parent("tbody").select("td")
                    ex_val = clean_text(tds[4].text)
                    row[f'ex{i}'] = float(ex_val) if ex_val and ex_val != "." else 0.0
                else: row[f'ex{i}'] = 0.0
            except: row[f'ex{i}'] = 0.0

            # [ç•ªçµ„è¡¨] è©³ç´°ãƒ‡ãƒ¼ã‚¿
            try:
                list_tbody = soup_list.select_one(f".is-boatColor{i}").find_parent("tbody")
                tds = list_tbody.select("td")
                
                # å‹ç‡
                wr_match = re.search(r"(\d\.\d{2})", clean_text(tds[3].text))
                row[f'wr{i}'] = float(wr_match.group(1)) if wr_match else 0.0
                
                # Fæ•° (é¸æ‰‹åæ¬„ tds[2] ã‹ã‚‰ "F1" ç­‰ã‚’æŠ½å‡º)
                f_match = re.search(r"F(\d+)", clean_text(tds[2].text))
                row[f'f{i}'] = int(f_match.group(1)) if f_match else 0
                
                # å¹³å‡ST (è¡Œå…¨ä½“ã‹ã‚‰ "ST0.15" ã‚’æ¢ã™)
                st_match = re.search(r"ST(\d\.\d{2})", list_tbody.text.replace("\n", ""))
                row[f'st{i}'] = float(st_match.group(1)) if st_match else 0.17
                
                # ãƒ¢ãƒ¼ã‚¿ãƒ¼2é€£ç‡ (tds[5] or tds[6] ã‹ã‚‰ "%" ã®ã¤ã„ãŸæ•°å­—ã‚’æŠ½å‡º)
                mo_text = clean_text(tds[5].text) # é€šå¸¸ã¯ã“ã“
                mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                if not mo_match:
                    mo_text = clean_text(tds[6].text) # å¿µã®ãŸã‚éš£ã‚‚ãƒã‚§ãƒƒã‚¯
                    mo_match = re.search(r"(\d{1,3}\.\d)", mo_text)
                row[f'mo{i}'] = float(mo_match.group(1)) if mo_match else 0.0
                
            except:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®å®‰å…¨å€¤
                row[f'wr{i}'], row[f'f{i}'], row[f'st{i}'], row[f'mo{i}'] = 0.0, 0, 0.20, 0.0

        print(" âœ… OK")
        return row

    except Exception as e:
        print(f" âŒ Error: {e}")
        return None

if __name__ == "__main__":
    # å¼•æ•°ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    parser = argparse.ArgumentParser()
    parser.add_argument("--start", default=DEFAULT_TARGET_DATE, help="å–å¾—é–‹å§‹æ—¥ (YYYYMMDD)")
    # endå¼•æ•°ã¯äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã™ãŒã€ã“ã®ãƒ¢ãƒ¼ãƒ‰ã§ã¯ä½¿ã‚ãªã„
    parser.add_argument("--end", default=None, help="çµ‚äº†æ—¥ (ç„¡è¦–ã•ã‚Œã¾ã™)") 
    args = parser.parse_args()

    target_date = args.start.replace("-", "")
    print(f"ğŸš€ DEBUG START: {target_date} (Limit: {MAX_RACES} races)")
    
    collected_data = []
    
    # 24å ´Ã—12Rã‚’å·¡å›
    for jcd in range(1, 25):
        if len(collected_data) >= MAX_RACES: break
        
        for rno in range(1, 13):
            if len(collected_data) >= MAX_RACES: break
            
            data = scrape_race(jcd, rno, target_date)
            if data:
                # ãƒ­ã‚°ç¢ºèªç”¨
                print(f"   -> Result: 1ç€={data.get('rank1')} / é…å½“:Â¥{data.get('payout')}")
                print(f"   -> 1å·è‰‡: F{data.get('f1')} / ST{data.get('st1')} / Mo{data.get('mo1')}%")
                collected_data.append(data)
                time.sleep(1) # è² è·è»½æ¸›
            
    # CSVä¿å­˜
    if collected_data:
        os.makedirs("data", exist_ok=True)
        df = pd.DataFrame(collected_data)
        
        # ã‚«ãƒ©ãƒ é †åºã‚’æ•´ç†
        cols = ['date', 'jcd', 'rno', 'wind', 'res1', 'rank1', 'rank2', 'rank3', 'payout']
        for i in range(1, 7):
            cols.extend([f'wr{i}', f'mo{i}', f'ex{i}', f'f{i}', f'st{i}'])
        
        # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã ã‘æŠ½å‡º
        use_cols = [c for c in cols if c in df.columns]
        df = df[use_cols]

        output_path = "data/debug_result.csv"
        df.to_csv(output_path, index=False)
        print(f"\nğŸ‰ å®Œäº†ï¼ {len(df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    else:
        print("\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥ä»˜ã‚„é–‹å‚¬ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
